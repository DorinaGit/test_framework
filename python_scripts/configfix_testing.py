# -*- coding: utf-8 -*-
"""Configfix testing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ewhvqvpjdddkx290qGj76mzum2ne6AFB
"""

# library imports
from itertools import chain 
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt 
import matplotlib.ticker as mticker

"""Data can be easily loaded from a CSV file on Github."""

url_1 = 'https://raw.githubusercontent.com/izimbra/configfix_test/master/tests_59/results_old.csv'
url_2 = 'https://raw.githubusercontent.com/izimbra/configfix_test/master/tests_59/results.csv'
url_p = 'https://raw.githubusercontent.com/izimbra/configfix_test/master/tests_59/results_x86-64_patrick.csv'


data_1 = pd.read_csv(url_1)
data_2 = pd.read_csv(url_2)
data_p = pd.read_csv(url_p)

data_2.columns

"""After that in can be filtered, aggregated, presented in table form, plotted etc."""

configs = data_1[['Architecture', 'KCONFIG_PROBABILITY', 'Symbol count', 
                     'Tristates', 'No. symbols = YES | MOD', 'No. conflict candidates']].drop_duplicates()
configs.head()

print(configs.shape[0], 'configuration samples generated for', configs.nunique()['Architecture'], 
      'architectures:\n', configs[['Architecture']].drop_duplicates().values)

"""### Statistics"""

rows = ['No. architectures', 'Architectures', 'No. config samples',
        'A: No. generated conflicts', 'B: No. conflicts with fixes', 
        'C: % conflicts with fixes', 'No. resolved conflicts', 
        '% resolved conflicts', 'D: No. fixes', 
        'No. result 1', 'E_1: % result 1', 
        'No. result 2', 'E_2: % result 2', 
        'No. result 3', 'E_3: % result 3']

def conflict_stats(dest, col, data):
  """
  Populates a column with conlict resolution metrics, defined by 'rows',
  in the destination DataFrame based on the selected data.

  Parameters:
  -----------
  dest : DataFrame
    Destination DataFrame, where a column should be filled with metrics.
  col : int
    Column index.
  data : DataFrame
    DataFrame containing the data to be used for calculations.
  """
  arch_list = list(chain.from_iterable(data[['Architecture']].drop_duplicates().values.tolist()))
  no_conflicts = data.nunique()['Conflict']
  no_conflicts_fixes = data[data['No. diagnoses'] > 0].nunique()['Conflict']
  no_conflicts_resolved = data.query(' Resolved == "YES" ').nunique()['Conflict']
  no_fixes = data[data['Diag. index'] != '-'].shape[0]
  # result 1 (optimal): fully applicable and resolves conflict
  no_result_1 = data.query(' Resolved=="YES" & Applied=="YES" ').shape[0]
  # result 2 (semi-optimal, fix is redundant): not fully applicable, but resolves conflict
  no_result_2 = data.query(' Resolved=="YES" & Applied=="NO" ').shape[0]
  # result 3: doesn't resolve conflict
  no_result_3 = data.query(' Resolved=="NO" & Applied=="NO" ').shape[0]

  dest.iloc[0,col] = data.nunique()['Architecture']
  dest.iloc[1,col] = arch_list
  dest.iloc[2,col] = data[['Architecture', 'KCONFIG_PROBABILITY']].drop_duplicates().shape[0]
  dest.iloc[3,col] = no_conflicts
  dest.iloc[4,col] = no_conflicts_fixes
  dest.iloc[5,col] = no_conflicts_fixes / no_conflicts * 100
  dest.iloc[6,col] = no_conflicts_resolved
  dest.iloc[7,col] = no_conflicts_resolved / no_conflicts * 100
  dest.iloc[8,col] = no_fixes
  dest.iloc[9,col] = no_result_1
  dest.iloc[10,col] = no_result_1 / no_fixes * 100
  dest.iloc[11,col] = no_result_2
  dest.iloc[12,col] = no_result_2 / no_fixes * 100
  dest.iloc[13,col] = no_result_3
  dest.iloc[14,col] = no_result_3 / no_fixes * 100


# overall testing statistics
stats_table = pd.DataFrame(index = rows,
                           columns = ['Overall test runs 1', 'x86_64 test run 1', 
                                      'x86_64 test run 2 (Eugene)', 'x86_64 test run 2 (Patrick)',
                                      'Overall test runs 2', 'Overall test runs 2 (excl. i386)'])



# overall stats for test runs 1
conflict_stats(stats_table, 0, data_1)

# x86_64 stats for test runs 1
conflict_stats(stats_table, 1, data_1.query(' Architecture == "x86_64" '))

# x86_64 stats for test runs 2
conflict_stats(stats_table, 2, data_2.query(' Architecture == "x86_64" '))

# x86_64 stats from Patrick
conflict_stats(stats_table, 3, data_p.query(' Architecture == "x86_64" '))

# overall stats for test runs 2
conflict_stats(stats_table, 4, data_2)

# overall stats for test runs 2 excl. i386
conflict_stats(stats_table, 5, data_2.query(' Architecture != "i386" '))

stats_table

"""#### Detailed OpenRISC statistics

We have test results for OpenRISC for conflicts sizes 1-10. For this architecture, the table shows the same metrics as above for each conflict size, along with their average values. 
"""

# detailed OpenRISC statistics
openrisc_table = pd.DataFrame(index = rows,
                              columns = ['average', 'sizes=1..3 (week 44)', 'size=1', 'size=2', 'size=3', 
                                         'size=4',  'size=5', 'size=6', 'size=7', 'size=8', 'size=9', 'size=10'])

conflict_stats(openrisc_table, 0, data_2.query(' Architecture == "openrisc" '))
conflict_stats(openrisc_table, 1, data_2.query(' Architecture == "openrisc" & `Conflict size` < 4 '))

for size in range(1,11):
  conflict_stats(openrisc_table, size+1, data_2.query(f' Architecture == "openrisc" & `Conflict size` == {size}'))

openrisc_table

"""#### Detailed ARM64 statistics"""

# detailed ARM64 statistics
arm64_table = pd.DataFrame(index = rows,
                              columns = ['average', 'sizes=1..3 (week 44)', 'size=1', 'size=2', 'size=3', 
                                         'size=4',  'size=5', 'size=6', 'size=7', 'size=8', 'size=9', 'size=10'])

conflict_stats(arm64_table, 0, data_2.query(' Architecture == "arm64" '))
conflict_stats(arm64_table, 1, data_2.query(' Architecture == "arm64" & `Conflict size` < 4 '))

for size in range(1,11):
  conflict_stats(arm64_table, size+1, data_2.query(f' Architecture == "arm64" & `Conflict size` == {size}'))

arm64_table

"""#### Detailed x86_64 statistics"""

# detailed x86_64 statistics
x86_64_table = pd.DataFrame(index = rows,
                              columns = ['average', 'sizes=1..3 (week 44)', 'size=1', 'size=2', 'size=3', 
                                         'size=4',  'size=5', 'size=6', 'size=7', 'size=8', 'size=9', 'size=10'])

conflict_stats(x86_64_table, 0, data_2.query(' Architecture == "x86_64" '))
conflict_stats(x86_64_table, 1, data_2.query(' Architecture == "x86_64" & `Conflict size` < 4 '))

for size in range(1,11):
  conflict_stats(x86_64_table, size+1, data_2.query(f' Architecture == "x86_64" & `Conflict size` == {size}'))

x86_64_table

"""### Shared diagram data"""

probs = range(10,100,10) # 10-90
conf_sizes = range(1,11) # 1-10

plot_data = data_2.query(' Architecture != "i386" ') # exclude incomplete i386 run
resolved_only = plot_data.query(' `Diag. size` != "-" ')

"""### Conflict resolution ratio in detail"""

xs = np.array(conf_sizes)
width=0.25

plt.figure(figsize=(12,6))
plt.xlim(0,11)
plt.xticks(xs)

for table, label, shift, color in zip([arm64_table, openrisc_table, x86_64_table],
                                      ['ARM64', 'OpenRISC', 'x86_64'],
                                      [-width, 0, +width],
                                      ['lightblue', 'silver', 'wheat']):
  plt.bar(x=xs+shift, height=table.iloc[7,2:12].values, width=width, label=label, color=color)


plt.axhline(stats_table.iloc[10,5], c='salmon', label='Overall average')

plt.title('Resolution ratio vs. conflict size')
plt.ylabel('Conflict resolution ratio, %')
plt.xlabel('Conflict size')
plt.legend()

plt.show()

# plot_data = data_2.query(' Architecture != "i386" ')
# fix_sizes = pd.to_numeric(plot_data[(plot_data['Diag. size'] != '-')]['Diag. size'])

# arch = 'openrisc'

# totals    = data_2.query(f' Architecture == "{arch}" ').groupby('KCONFIG_PROBABILITY').nunique()['Conflict'].to_numpy()
# # resolveds = data_2.query(f' `No. diagnoses` > 0 & Architecture == "{arch}" ').groupby('KCONFIG_PROBABILITY').nunique()['Conflict'].to_numpy()
# # resolveds/totals
#           # (plot_data['Resolution time'] >= 300.0)].groupby('Conflict size').count()['Conflict']

# resolveds = selection.query(' `No. diagnoses` > 0 ').groupby('KCONFIG_PROBABILITY').nunique()['Conflict'].to_numpy()
# resolveds / totals

# data_2.query(f' Architecture == "{arch}" ').groupby('KCONFIG_PROBABILITY').nunique()['Conflict'].to_numpy()

xs = np.array(probs)
width=2

plt.figure(figsize=(12,6))
plt.xlim(0,100)
plt.xticks(xs)

for table, label, shift, color in zip([arm64_table, openrisc_table, x86_64_table],
                                      ['ARM64', 'OpenRISC', 'x86_64'],
                                      [-width, 0, +width],
                                      ['lightblue', 'silver', 'wheat']):
  arch = label.lower()
  selection = data_2.query(f' Architecture == "{arch}" ')
  # no. conflicts for each probability
  totals    = selection.groupby('KCONFIG_PROBABILITY').nunique()['Conflict'].to_numpy()
  # no. resolved conflicts for each probability
  resolveds = selection.query(' `No. diagnoses` > 0 ').groupby('KCONFIG_PROBABILITY').nunique()['Conflict'].to_numpy()
  resolveds / totals
  plt.bar(x=xs+shift, height=resolveds / totals * 100, width=width, label=label, color=color)

print( data_2.query(' Architecture != "i386" ').nunique()['Conflict'] )
print( data_2.query(' Architecture != "i386" & `No. diagnoses` > 0 ').nunique()['Conflict'] )
print( 1055 / 1350 )
avg = stats_table.iloc[10,5]
plt.axhline(avg, c='salmon', label='Overall average')

plt.title('Resolution ratio vs. probability')
plt.ylabel('Conflict resolution ratio, %')
plt.xlabel('KCONFIG_PROBABILITY')
plt.legend()

plt.show()

"""### Fix sizes"""

fix_sizes = pd.to_numeric(resolved_only['Diag. size'])
unique_sizes = fix_sizes.sort_values().unique()

print('RangeFix returned fixes of', fix_sizes.nunique(), 'distinct sizes:\n', unique_sizes)

"""#### Histograms with distinct bars per size"""

# distribution statistics
mean = np.int(np.mean(fix_sizes))
median = np.int(np.median(fix_sizes))

fig2, ax2_1 = plt.subplots(1, 1, figsize=(20, 7))

# show whole value range of fix sizes
size_range = list(range(unique_sizes.min(), unique_sizes.max()+4))
N, bins, patches = ax2_1.hist(fix_sizes, bins=size_range)

# highlight mean and median
for bin, patch in zip(bins, patches):
  if bin == median:
    patch.set_facecolor('salmon') 
    patch.set_label('Median = 20')
  if bin == mean:
    patch.set_facecolor('lightskyblue')
    patch.set_label('Mean = 23')

ax2_1.xaxis.set_major_locator(mticker.IndexLocator( base = 4, offset = 0))
ax2_1.xaxis.set_minor_locator(mticker.IndexLocator( base = 1, offset = 0))
ax2_1.tick_params(labelsize=11)
ax2_1.xaxis.grid(True, which='both')
ax2_1.set_xlabel('Fix size', fontsize=16)
ax2_1.set_ylabel('Frequency', fontsize=16)
# ax2_1.set_title('Histogram, whole value range of fix sizes', fontsize=14)

# # skip missing fix size
# ax2_2.hist(fix_sizes, bins=unique_sizes)
# ax2_2.xaxis.set_major_locator(mticker.IndexLocator( base = 4, offset = 0))
# ax2_2.xaxis.set_minor_locator(mticker.IndexLocator( base = 1, offset = 0))
# ax2_2.xaxis.grid(True, which='both')
# ax2_2.set_xlabel('Fix size', fontsize=12)
# ax2_2.set_ylabel('Frequency', fontsize=12)
# ax2_2.set_title('Histogram, present fix sizes only', fontsize=14)
plt.legend(loc='upper center', fontsize=14)
plt.show()

# quartiles and percentiles
q25, q75 = np.percentile(fix_sizes, [25, 75])
print(f'50% of the fixes have sizes between {q25} and {q75} (interquartile range)')
p10, p90 = np.percentile(fix_sizes, [10, 90])
print(f'80% of the fixes have sizes between {p10} and {p90}')

bins



"""#### Zooming in on the majority of the fixes"""

# fig3 = plt.figure(figsize=(15,7))
# range_50 = list(range(unique_sizes.min(), 50))
# plt.hist(fix_sizes, bins=range_50)
# loc3 = mticker.IndexLocator( base = 1, offset = 0) 
# fig3.axes[0].xaxis.set_major_locator(loc3)
# fig3.axes[0].xaxis.grid(True)
# # fig3.autofmt_xdate()

# plt.xlabel('Fix size', fontsize=12)
# plt.ylabel('Frequency', fontsize=12)
# plt.title('Histogram of fix sizes between 2-50')
# plt.show()

"""#### Scatter plot"""

conflict_sizes = pd.to_numeric(resolved_only['Conflict size'], downcast='unsigned')
ticks = list(conf_sizes)

fig4, (ax4_1, ax4_2) = plt.subplots(1, 2, figsize=(15,6))

ax4_1.set_xticks(ticks) 
ax4_1.yaxis.grid(True)
ax4_1.set_xlabel('Conflict size', fontsize=12)
ax4_1.set_ylabel('Fix size', fontsize=12)
ax4_1.scatter(conflict_sizes, fix_sizes, alpha=0.5)

ax4_2.set_yticks(ticks) 
ax4_2.xaxis.grid(True)
ax4_2.set_ylabel('Conflict size', fontsize=12)
ax4_2.set_xlabel('Fix size', fontsize=12)
ax4_2.scatter(fix_sizes, conflict_sizes, alpha=0.5)

plt.show()

"""#### Box plot"""

# group fix sizes by conflict sizes, extract each group
grouped_by_conf_size = resolved_only.groupby(['Conflict size'])
diag_sizes = []

for conf_size in conf_sizes:
  diag_sizes.append(pd.to_numeric(grouped_by_conf_size.get_group(conf_size)['Diag. size']))

fig5, ax5 = plt.subplots(figsize=(10,10))
ax5.boxplot(np.asarray(diag_sizes, dtype=object)) # specify data type to avoid warnings
ax5.yaxis.grid(True, alpha=0.5)
ax5.tick_params(labelsize=12)
ax5.set_xlabel('Conflict size', fontsize=20)
ax5.set_ylabel('Fix size', fontsize=20)

plt.show()

"""#### Combined box plot"""

combined = [fix_sizes]
groups = resolved_only.groupby(['Conflict size'])
xticks = ['All'] 
for conf_size, group in groups:
  combined.append( pd.to_numeric(group['Diag. size']) )
  xticks.append(str(conf_size))

fig5a, ax5a = plt.subplots(figsize=(10,10))
ax5a.boxplot(np.asarray(combined, dtype=object)) # specify data type to avoid warnings
ax5a.yaxis.grid(True, alpha=0.5)
ax5a.tick_params(labelsize=12)
ax5a.set_xticklabels(xticks)

ax5a.set_xlabel('Conflict size', fontsize=18)
ax5a.set_ylabel('Fix size', fontsize=18)

plt.show()

"""##### Y axis cut off at 75"""

fig5b, ax5b = plt.subplots(figsize=(10,6))
ax5b.boxplot(np.asarray(combined, dtype=object)) # specify data type to avoid warnings
ax5b.set_ylim(0,75)
ax5b.set_yticks(range(0,76,15))
ax5b.yaxis.grid(True, alpha=0.5)
ax5b.tick_params(labelsize=12)
ax5b.set_xticklabels(xticks)

ax5b.set_xlabel('Conflict size', fontsize=20)
ax5b.set_ylabel('Fix size', fontsize=20)

plt.show()

"""#### Violin plots"""

# overall distribution - alternative to the histogram
# fig4alt, ax2_1 = plt.subplots(1, 1, figsize=(20, 7))
fig4a, ax4a = plt.subplots(figsize=(8,8))

plot = ax4a.violinplot(fix_sizes, 
                       showmeans=True, 
                       showmedians=True, 
                       quantiles=[0.25, 0.75])
ax4a.set_xticks(ticks=[])
ax4a.tick_params(labelsize=12)
ax4a.set_ylabel('Fix sizes', fontsize=20)

plot['cmedians'].set_color('tomato')
plot['cmedians'].set_label('Median = 20')
# ax4a.annotate('meadian = 20', tuple(plot['cmedians'].get_paths()[0].vertices[1]))

plot['cmeans'].set_color('limegreen')
plot['cmeans'].set_label('Mean = 23')
plot['cmeans'].get_paths()[0].vertices[1]
# ax4a.annotate('mean = 23', (plot['cmeans'].get_paths()[0].vertices[1]))

plt.legend()
plt.show()

# fig5a, ax5a = plt.subplots(figsize=(8,10))
# # vecs = diag_sizes
# # vecs.append(fix_sizes)
# plot = ax5a.violinplot(diag_sizes, 
#                        showmeans=True, 
#                        showmedians=True) #, quantiles=[0.25, 0.75])
plot['cmedians'].get_paths()[0].vertices[1]

"""### Running times"""

no_resolved = resolved_only.nunique()['Conflict']

# select data that pertain to conflicts only
resolved_conflicts = resolved_only.loc[:,['Architecture','KCONFIG_PROBABILITY','Tristates','Conflict','Conflict size','Resolution time']]
resolved_conflicts.drop_duplicates(inplace=True)

assert resolved_conflicts.index.size == no_resolved

"""#### Distribution statistics

Results below include only the resolved conflicts.
"""

# for our particular result, all running times happen to be unique
running_times_resolved = pd.to_numeric(resolved_only['Resolution time'].unique())
assert running_times_resolved.size == no_resolved

min_time = np.min(running_times_resolved)
max_time = np.max(running_times_resolved)
mean_time = np.mean(running_times_resolved)
median_time = np.median(running_times_resolved)

p70, p75, p80, p90 = np.percentile(running_times_resolved, [70, 75, 80, 90])

print(f'Min: {min_time}\nMax: {max_time}\nMean: {mean_time}\nMedian: {median_time}')
print(f'Percentiles:\n\t70th: {p70}\n\t75th: {p75}\n\t80th: {p80}\n\t90th: {p90}')

"""#### No. timeouts per conflict size"""

plot_data.query(' `Diag. size` == "-" & `Resolution time` >= 300.0 ').groupby(
    'Conflict size').count()['Conflict']

"""#### Box plot - running times by conflict size"""

# group running times by conflict sizes
groups = resolved_conflicts.groupby(['Conflict size'])
vectors = []

for size, group in groups:
  vectors.append( pd.to_numeric(group['Resolution time']) )

fig6, ax6 = plt.subplots(figsize=(8,8)) #(10,10))
ax6.boxplot(np.asarray(vectors, dtype=object)) # specify data type to avoid warnings
ax6.yaxis.grid(True, alpha=0.5)
ax6.tick_params(labelsize=12)
ax6.set_xlabel('Conflict size', fontsize=14)
ax6.set_ylabel('Resolution time', fontsize=14)

x = pd.to_numeric(resolved_conflicts['Conflict size'])
y = pd.to_numeric(resolved_conflicts['Resolution time'])
assert x.size == no_resolved
assert y.size == no_resolved
coef = np.polyfit(x, y, deg=1)
regr_fn = np.poly1d(coef)
plt.plot(x, regr_fn(x), c='red', label='Linear regression')
plt.legend()
plt.title('Resolution time by conflict size', fontdict={'fontsize':16})

plt.show()

"""#### Box plot - running times by probability"""

# group running times by probability
groups = resolved_conflicts.groupby(['KCONFIG_PROBABILITY'])
vectors = []

for prob, group in groups:
  vectors.append( pd.to_numeric(group['Resolution time']) )

fig7, ax7 = plt.subplots(figsize=(8,8)) #(10,10))

ax7.yaxis.grid(True, alpha=0.5)
ax7.tick_params(labelsize=12)
ax7.set_xlabel('KCONFIG_PROBABILITY', fontsize=14)
ax7.set_ylabel('Resolution time', fontsize=14)
ax7.boxplot(np.asarray(vectors, dtype=object), # specify data type to avoid warnings
            labels=list(probs)) 

x = pd.to_numeric(resolved_conflicts['KCONFIG_PROBABILITY'])/10
y = pd.to_numeric(resolved_conflicts['Resolution time'])
assert x.size == no_resolved
assert y.size == no_resolved
coef = np.polyfit(x, y, deg=1)
regr_fn = np.poly1d(coef)
plt.plot(x, regr_fn(x), c='red', label='Linear regression')
plt.legend()
plt.title('Resolution time by probability', fontdict={'fontsize':16})

plt.show()

"""#### Multiple factors

##### Probability, tristates, conflict size
"""

tristates = ['YES', 'NO']
rows = range(len(tristates))
cols = range(len(probs))

# group data
groups = resolved_conflicts.groupby(['Tristates','KCONFIG_PROBABILITY','Conflict size'])
assert groups.size().sum() == no_resolved

fig8, axs = plt.subplots(figsize=(25,8),
                         nrows=len(rows), 
                         ncols=len(cols), 
                         sharex='col',
                         sharey='row',
                         gridspec_kw={"hspace":0.1, "wspace":0.1})
num_points = 0

for row, tri in zip(rows, tristates):
  for col, prob in zip(cols, probs):
    ax = axs[row][col]
    ax.set_ylim(0, max_time * 1.1)

    # select subplot data
    vectors = []
    num_subplot_points = 0
    for conf_size in conf_sizes:
      try:
        group = groups.get_group((tri,prob,conf_size))['Resolution time']
        num_subplot_points += group.count()
        vectors.append( pd.to_numeric(group) )
      except KeyError:
        pass

    # box plot
    ax.boxplot(np.asarray(vectors, dtype=object)) # specify data type to avoid warnings
    ax.set_xlabel(f'{num_subplot_points} points')
    num_points += num_subplot_points

    # regression line
    if num_subplot_points > 0:
      selected = resolved_conflicts.query(f' Tristates == "{tri}" & KCONFIG_PROBABILITY == {prob}')
      assert selected.index.size == num_subplot_points
      x = selected['Conflict size']
      y = selected['Resolution time']
      coef = np.polyfit(x, y, deg=1)
      regr_fn = np.poly1d(coef)
      ax.plot(x, regr_fn(x), c='red')

assert num_points == no_resolved

# figure labels
fig8.text(0.1, 0.5, 'Resolution time, sec.', va='center', rotation='vertical', fontsize=14)
fig8.text(0.91, 0.7, 'Tristates=YES', va='center', rotation='vertical', fontsize=14)
fig8.text(0.91, 0.3, 'Tristates=NO', va='center', rotation='vertical', fontsize=14)
fig8.text(0.5, 0.01, 'KCONFIG_PROBABILITY', ha='center', fontsize=14)
for prob in probs:
  fig8.text(0.05 + prob/110, 0.05, f'{prob} %', ha='center', fontsize=14)

plt.show()

"""##### Probability, architecture, conflict size"""

archs = ['arm64', 'openrisc', 'x86_64']
rows = range(len(archs))
cols = range(len(probs))

# group data
groups = resolved_conflicts.groupby(['Architecture','KCONFIG_PROBABILITY','Conflict size'])
assert groups.size().sum() == no_resolved

fig9, axs = plt.subplots(figsize=(25,10),
                         nrows=len(rows), 
                         ncols=len(cols), 
                         sharex='col',
                         sharey='row',
                         gridspec_kw={"hspace":0.1, "wspace":0.1})

num_points = 0

for row, arch in zip(rows, archs):
  for col, prob in zip(cols, probs):
    ax = axs[row][col]
    ax.set_ylim(0, max_time * 1.1)

    # select subplot data
    vectors = []
    num_subplot_points = 0
    for conf_size in conf_sizes:
      try:
        group = groups.get_group((arch,prob,conf_size))['Resolution time']
        num_subplot_points += group.count()
        vectors.append( pd.to_numeric(group) )
      except KeyError:
        pass

    # box plot
    ax.boxplot(np.asarray(vectors, dtype=object)) # specify data type to avoid warnings
    ax.set_xlabel(f'{num_subplot_points} points')
    num_points += num_subplot_points

    # regression line
    if num_subplot_points > 0:
      selected = resolved_conflicts.query(f' Architecture == "{arch}" & KCONFIG_PROBABILITY == {prob}')
      assert selected.index.size == num_subplot_points
      x = selected['Conflict size']
      y = selected['Resolution time']
      coef = np.polyfit(x, y, deg=1)
      regr_fn = np.poly1d(coef)
      ax.plot(x, regr_fn(x), c='red')

assert num_points == no_resolved

# figure labels
fig9.text(0.1, 0.5, 'Resolution time, sec.', va='center', rotation='vertical', fontsize=14)
for arch, y in zip(archs, [0.75, 0.5, 0.25]):
  fig9.text(0.91, y, arch, va='center', rotation='vertical', fontsize=14)
fig9.text(0.5, 0.01, 'KCONFIG_PROBABILITY', ha='center', fontsize=14)
for prob in probs:
  fig9.text(0.05 + prob/110, 0.05, f'{prob} %', ha='center', fontsize=14)

plt.show()

"""### Save figures"""

from google.colab import files

# fig2.savefig('fix_sizes_histogram_font16_mean_median.pdf')
# fig5.savefig('fig5_font20.pdf')
fig5b.savefig('fig5_6_combined_box_75_font20.pdf')

# fig2.savefig('scatterplot.pdf')

